# ═══════════════════════════════════════════════════════════════════════
# Cognitive Core — LLM Provider Configuration
# ═══════════════════════════════════════════════════════════════════════
#
# This file externalizes all provider/model mappings from Python code.
# To switch providers: change `default_provider` or set LLM_PROVIDER env var.
# To change models: update the alias table below.
# No Python code changes required.
#
# Environment variable overrides (highest priority):
#   LLM_PROVIDER       — overrides default_provider
#   LLM_DEFAULT_MODEL  — overrides the "default" alias for the active provider
# ═══════════════════════════════════════════════════════════════════════

# Which provider to use when no env var or CLI flag is set.
# Values: google | azure | azure_foundry | openai | bedrock
default_provider: azure_foundry

# ── Model Alias Table ────────────────────────────────────────────────
# Workflow YAML uses logical names (default, fast, standard, strong).
# This table maps them to provider-specific model IDs.
# Add new tiers (e.g., "reasoning", "vision") as needed.

aliases:
  default:
    google:         gemini-2.0-flash
    azure:          gpt-4o-mini
    azure_foundry:  gpt-4o-mini
    openai:         gpt-4o-mini
    bedrock:        anthropic.claude-3-5-haiku-20241022-v1:0

  fast:
    google:         gemini-2.0-flash
    azure:          gpt-4o-mini
    azure_foundry:  gpt-4o-mini
    openai:         gpt-4o-mini
    bedrock:        anthropic.claude-3-5-haiku-20241022-v1:0

  standard:
    google:         gemini-2.5-pro
    azure:          gpt-4o
    azure_foundry:  gpt-4o
    openai:         gpt-4o
    bedrock:        anthropic.claude-3-5-sonnet-20241022-v2:0

  strong:
    google:         gemini-2.5-pro
    azure:          gpt-4o
    azure_foundry:  gpt-4o
    openai:         gpt-4o
    bedrock:        anthropic.claude-3-5-sonnet-20241022-v2:0

# ── Reverse Lookup ───────────────────────────────────────────────────
# Maps provider-specific model names → provider.
# Used when CLI passes a raw model name (e.g., --model gpt-4o).

model_to_provider:
  # Google
  gemini-2.0-flash:   google
  gemini-2.5-pro:     google
  gemini-1.5-pro:     google
  gemini-1.5-flash:   google
  # OpenAI / Azure
  gpt-4o:             openai
  gpt-4o-mini:        openai
  gpt-4.1:            openai
  gpt-4.1-mini:       openai
  gpt-4.1-nano:       openai
  o3:                 openai
  o3-mini:            openai
  o4-mini:            openai
  # Anthropic / Bedrock
  claude-3-5-sonnet:  bedrock
  claude-3-5-haiku:   bedrock
  claude-sonnet-4:    bedrock

# ── Provider Defaults ────────────────────────────────────────────────
# Fallback model when alias resolution produces nothing.

provider_defaults:
  google:         gemini-2.0-flash
  azure:          gpt-4o
  azure_foundry:  gpt-4o
  openai:         gpt-4o
  bedrock:        anthropic.claude-3-5-sonnet-20241022-v2:0

# ── Provider-Specific Settings ───────────────────────────────────────
# Optional per-provider config. Env vars always override these.

provider_settings:
  azure:
    api_version: "2024-12-01-preview"
  azure_foundry:
    api_version: "2024-12-01-preview"
  bedrock:
    region: us-east-1

# ── Retry & Fallback ───────────────────────────────────────────────
# Per-provider retry config. Same-provider fallback (Option B).

retry:
  default:
    max_attempts: 3
    backoff_base: 1.0
    backoff_max: 30.0
    jitter: 0.2
    circuit_breaker_threshold: 5
    circuit_breaker_reset_seconds: 60.0
    retry_on_parse_failure: true

  google:
    max_attempts: 3
    backoff_base: 1.0
    fallback_model: gemini-2.5-pro

  azure:
    max_attempts: 3
    backoff_base: 1.5
    fallback_model: gpt-4o

  azure_foundry:
    max_attempts: 3
    backoff_base: 1.5
    fallback_model: gpt-4o

  openai:
    max_attempts: 3
    backoff_base: 1.0
    fallback_model: gpt-4o

  bedrock:
    max_attempts: 3
    backoff_base: 2.0
    fallback_model: anthropic.claude-3-5-sonnet-20241022-v2:0

# ── Rate Limits ─────────────────────────────────────────────────────
# Per-provider concurrency and request rate limits.

rate_limits:
  default:
    max_concurrent: 10
    requests_per_minute: 60
    queue_timeout: 30.0

  google:
    max_concurrent: 10
    requests_per_minute: 60

  azure:
    max_concurrent: 20
    requests_per_minute: 100

  azure_foundry:
    max_concurrent: 20
    requests_per_minute: 100

  openai:
    max_concurrent: 15
    requests_per_minute: 80

  bedrock:
    max_concurrent: 10
    requests_per_minute: 40

# ── Pricing (per million tokens) ────────────────────────────────────
# Co-located with provider config (Design: Option A).
# Update when models or pricing change.

pricing:
  # Google
  gemini-2.0-flash:
    input_per_million: 0.10
    output_per_million: 0.40
  gemini-2.5-pro:
    input_per_million: 1.25
    output_per_million: 10.00

  # Azure / OpenAI
  gpt-4o:
    input_per_million: 2.50
    output_per_million: 10.00
  gpt-4o-mini:
    input_per_million: 0.15
    output_per_million: 0.60
  gpt-4.1:
    input_per_million: 2.00
    output_per_million: 8.00
  gpt-4.1-mini:
    input_per_million: 0.40
    output_per_million: 1.60

  # Bedrock / Anthropic
  anthropic.claude-3-5-sonnet-20241022-v2:0:
    input_per_million: 3.00
    output_per_million: 15.00
  anthropic.claude-3-5-haiku-20241022-v1:0:
    input_per_million: 0.80
    output_per_million: 4.00
